{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lGH3FTM6OFwuKJ8vOfmoz2JBtOc70fAU",
      "authorship_tag": "ABX9TyNXS4Vo4lrS26w9Tjjtr1hO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdH66/Colab_notebooks/blob/main/Multi_image_augmentation_PyTorchandResNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-image augmentation and tuning Script with ResNet**\n",
        "\n",
        "1.   Connect colab notebook to google drive\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ke4g3mRzGYMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiA5tqk0GSiU",
        "outputId": "4db20e08-35d6-4966-8a42-45170781de76"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inport libraries"
      ],
      "metadata": {
        "id": "4trKX4L2rcRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch .nn as nn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "VXesg2KmrayD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Setup transform functions for agumentation and normalisation of training and validation data sets. Convert to tensors. Now use Pytorch library not PIL ibrary"
      ],
      "metadata": {
        "id": "r4kgflvXP8MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'Train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "      ]),\n",
        "      'Val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "      ]),\n",
        "}"
      ],
      "metadata": {
        "id": "inUQP2p_L6h6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Load data loaders and link to data in google drive"
      ],
      "metadata": {
        "id": "rglno4PAQm1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data path\n",
        "data_dir = \"/content/drive/My Drive/Colab Notebooks/Colab Classification/Data/Classification Dataset/\"\n",
        "#Load data\n",
        "Datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                    data_transforms[x])\n",
        "for x in ['Train', 'Val']}\n",
        "\n",
        "#Define Train and Validation Dataloaders\n",
        "dataloaders = {x:torch.utils.data.DataLoader(Datasets[x], batch_size=5,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "for x in ['Train', 'Val']}\n",
        "\n",
        "datasets_sizes = {x: len(Datasets[x]) for x in ['Train', 'Val']}\n",
        "print(datasets_sizes)\n",
        "\n",
        "class_names = Datasets['Val'].classes\n",
        "print(class_names)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KB2yX5wPG95",
        "outputId": "aaa13358-f41d-491a-93df-677a2f176ad3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Train': 303, 'Val': 49}\n",
            "['Bread', 'Coffee', 'Pasta']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Import pre-trained models from PyTorch"
      ],
      "metadata": {
        "id": "_I461Eo-R53F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "dir(models)"
      ],
      "metadata": {
        "id": "TgEeGcanQZAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Chose prefered model (e.g ResNet18, Resnet101, AlexNex)"
      ],
      "metadata": {
        "id": "0EVe2y7vcJJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet152 = models.resnet152(pretrained=True)\n",
        "resnet152.eval()\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "resnet50.eval()"
      ],
      "metadata": {
        "id": "Cdbsg3WYSLJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Pass the image to the model"
      ],
      "metadata": {
        "id": "uVoVSCMTSfF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output152 = resnet152(InputImg_bt)\n",
        "output50 = resnet50(InputImg_bt)"
      ],
      "metadata": {
        "id": "rKMvUqLdSeja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Read the image net classes into the ImageNet variable"
      ],
      "metadata": {
        "id": "vAZah0tuXXUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/My Drive/Colab Notebooks/Colab Classification/Data/imagenet1000Classes.txt\") as classesfile:\n",
        "  ImageNetclasses = [line.strip() for line in classesfile.readlines()]"
      ],
      "metadata": {
        "id": "MTf6-o_WS_8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Find the predicted index of the top scores that occur in the output of the model. Fetch the class name of maximum prediction and print with the probability."
      ],
      "metadata": {
        "id": "u0kK_w0WXmL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, predictedLabels = torch.sort(output152, descending=True)\n",
        "percentage = torch.sigmoid(output152)[0] * 100\n",
        "[(ImageNetclasses[index], percentage[index].item()) for index in predictedLabels[0][:10]] # Number is decending list of n top scores\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bm4eTB4W3qP",
        "outputId": "c81cb00c-2290-4277-a81c-d89ff6266f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"281: 'tabby, tabby cat',\", 99.99172973632812),\n",
              " (\"282: 'tiger cat',\", 99.99012756347656),\n",
              " (\"285: 'Egyptian cat',\", 99.9723129272461),\n",
              " (\"357: 'mink',\", 99.82244110107422),\n",
              " (\"298: 'mongoose',\", 99.80396270751953),\n",
              " (\"333: 'hamster',\", 99.80279541015625),\n",
              " (\"356: 'weasel',\", 99.76910400390625),\n",
              " (\"674: 'mousetrap',\", 99.72803497314453),\n",
              " (\"508: 'computer keyboard, keypad',\", 99.70501708984375),\n",
              " (\"673: 'mouse, computer mouse',\", 99.65232849121094)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, predictedLabels = torch.sort(output50, descending=True)\n",
        "percentage = torch.sigmoid(output50)[0] * 100\n",
        "[(ImageNetclasses[index], percentage[index].item()) for index in predictedLabels[0][:10]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtPOqFwyrzzq",
        "outputId": "d0bc15f0-cd5a-489d-8cfc-11467ea28059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"282: 'tiger cat',\", 99.99964141845703),\n",
              " (\"281: 'tabby, tabby cat',\", 99.9996337890625),\n",
              " (\"285: 'Egyptian cat',\", 99.99813842773438),\n",
              " (\"588: 'hamper',\", 99.96511840820312),\n",
              " (\"620: 'laptop, laptop computer',\", 99.94973754882812),\n",
              " (\"283: 'Persian cat',\", 99.93683624267578),\n",
              " (\"478: 'carton',\", 99.93658447265625),\n",
              " (\"728: 'plastic bag',\", 99.93647003173828),\n",
              " (\"463: 'bucket, pail',\", 99.92802429199219),\n",
              " (\"904: 'window screen',\", 99.92015838623047)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}